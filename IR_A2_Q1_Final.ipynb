{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = stopwords.words('english')\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xFU-w7094kt",
        "outputId": "8b933ece-e87a-40bf-d0fe-c15e609f41dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "corpus = []\n",
        "files = os.listdir('/content/drive/MyDrive/IR Assignments/A2/preprocessed')\n",
        "\n",
        "for i in files:\n",
        "    path = '/content/drive/MyDrive/IR Assignments/A2/preprocessed/' + i\n",
        "    f = open(path, 'r')\n",
        "    for i in f:\n",
        "        corpus.append(i)"
      ],
      "metadata": {
        "id": "SIo2Pjxx3Ayj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_tokens = []\n",
        "for i in corpus:\n",
        "    tokens = i.split()\n",
        "    doc_tokens.append(tokens)\n"
      ],
      "metadata": {
        "id": "5ELdVArC4clg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc_tokens)"
      ],
      "metadata": {
        "id": "N6v34x_e4nwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dfcb25-8931-47de-ed00-8a3cc96ed799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1399"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extraction(f):\n",
        "    new_text = ''\n",
        "    read = False\n",
        "        \n",
        "    for line in f:\n",
        "        line = line.strip()        \n",
        "        if line == '</TITLE>' or line == '</TEXT>':\n",
        "            read = False\n",
        "        if read:\n",
        "            new_text += (line + ' ')\n",
        "        if line == '<TITLE>' or line == '<TEXT>':\n",
        "            read = True\n",
        "      \n",
        "    return new_text"
      ],
      "metadata": {
        "id": "PRBgdcwg9r_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(x):\n",
        "    return x.lower()"
      ],
      "metadata": {
        "id": "G8UBVHWi9uOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    return nltk.word_tokenize(x) "
      ],
      "metadata": {
        "id": "GXdIyCRL9wqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(x):\n",
        "    temp_file = []\n",
        "\n",
        "    for i in x:\n",
        "        if i in stopwords:\n",
        "            continue\n",
        "        temp_file.append(i)\n",
        "    \n",
        "    s = ''\n",
        "    for i in temp_file:\n",
        "        s+=(i+ ' ')\n",
        "    return s"
      ],
      "metadata": {
        "id": "sCYJQQof9xkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(s):            \n",
        "    s = re.sub(r'[^\\w\\s]','',s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "Ibzcero99zIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_spaces(s):\n",
        "    return s.strip()"
      ],
      "metadata": {
        "id": "0qhy95DM90RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(s):\n",
        "    s = lowercase(s)\n",
        "    s = tokenize(s)\n",
        "    s = remove_stopwords(s)\n",
        "    s = remove_punctuations(s)\n",
        "    s = remove_spaces(s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "t5lJXtgN92SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Tkm5Pc2hF6"
      },
      "outputs": [],
      "source": [
        "def raw_tf(doc_tokens):\n",
        "    raw_tf = {}\n",
        "    for i in range(len(doc_tokens)):\n",
        "        raw_tf[i] = {}\n",
        "        unique_toks, tok_freq = np.unique(doc_tokens[i], return_counts=True)\n",
        "        x = 0\n",
        "        for j in unique_toks:\n",
        "            raw_tf[i][j] = tok_freq[x]\n",
        "            x+=1\n",
        "    \n",
        "    return raw_tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def no_of_docs_containing_word(corpus, word):\n",
        "    ans = 0\n",
        "    for i in corpus:\n",
        "        if word in i.split():\n",
        "            ans+=1     \n",
        "    return ans       "
      ],
      "metadata": {
        "id": "EGtpNgi9E9vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(corpus, vocab):\n",
        "    idf = {}\n",
        "    for j in tqdm(range(len(vocab))):\n",
        "        word = vocab[j]\n",
        "        freq = no_of_docs_containing_word(corpus,word)        \n",
        "        idf[word] = math.log10(len(corpus) / freq)        \n",
        "    return idf"
      ],
      "metadata": {
        "id": "Qew3XGcICIHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_weight(term, doc_tfs, scheme):\n",
        "    if(scheme == \"binary\"): \n",
        "        if(term in doc_tfs.keys()): \n",
        "            return 1\n",
        "        else: \n",
        "            return 0\n",
        "    elif(scheme == \"raw\"): \n",
        "        if(term in doc_tfs.keys()): \n",
        "            return doc_tfs[term]\n",
        "        else: \n",
        "            return 0\n",
        "    elif(scheme == \"tf\"):\n",
        "        if(term in doc_tfs.keys()): \n",
        "            total_terms = sum(doc_tfs.values())\n",
        "            return doc_tfs[term] / total_terms\n",
        "        else: \n",
        "            return 0\n",
        "    elif(scheme == \"log_norm\"): \n",
        "        if(term in doc_tfs.keys()): \n",
        "            return math.log10(1 + doc_tfs[term])\n",
        "        else: \n",
        "            return 0\n",
        "    elif(scheme == \"double_norm\"): \n",
        "        if(term in doc_tfs.keys()): \n",
        "            t1 = 0.5\n",
        "            t2 = (0.5)*(doc_tfs[term] / max(doc_tfs.values()))\n",
        "            return t1 + t2\n",
        "        else: \n",
        "            return 0.5"
      ],
      "metadata": {
        "id": "ETOaiX0m_N4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_tf_idf(doc_tokens, vocab, scheme):\n",
        "    num_docs = len(doc_tokens)\n",
        "\n",
        "    raw_tfs = raw_tf(doc_tokens)\n",
        "    term_idfs = compute_idf(corpus, vocab)\n",
        "\n",
        "    num_words = len(vocab)\n",
        "\n",
        "    tf_idf = np.zeros((num_docs, num_words))\n",
        "\n",
        "    for i in tqdm(range(num_docs)):\n",
        "        x = 0\n",
        "        for j in vocab:            \n",
        "            term_weight = tf_weight(j, raw_tfs[i], scheme)\n",
        "            idf = term_idfs[j]\n",
        "            tf_idf[i][x] = term_weight*idf\n",
        "            x+=1\n",
        "\n",
        "    return tf_idf\n"
      ],
      "metadata": {
        "id": "T-JZwn0L_cJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "vocab = []\n",
        "for i in doc_tokens:\n",
        "    for word in i:\n",
        "        vocab.append(word)\n",
        "vocab = list(set(vocab))"
      ],
      "metadata": {
        "id": "X_XL2OtP7tvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_tf_idf = gen_tf_idf(doc_tokens, vocab, 'binary')\n",
        "raw_tf_idf = gen_tf_idf(doc_tokens, vocab, 'raw')\n",
        "tf_tf_idf = gen_tf_idf(doc_tokens, vocab, 'tf')\n",
        "log_norm_tf_idf = gen_tf_idf(doc_tokens, vocab, 'log_norm')\n",
        "double_norm_tf_idf  = gen_tf_idf(doc_tokens, vocab, 'double_norm')"
      ],
      "metadata": {
        "id": "3Bs2RsdIBIXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d55e7b-99ec-4dd3-9b19-d4219208922c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8960/8960 [01:41<00:00, 88.27it/s]\n",
            "100%|██████████| 1399/1399 [00:10<00:00, 129.42it/s]\n",
            "100%|██████████| 8960/8960 [01:42<00:00, 87.64it/s]\n",
            "100%|██████████| 1399/1399 [00:13<00:00, 106.31it/s]\n",
            "100%|██████████| 8960/8960 [01:44<00:00, 85.88it/s]\n",
            "100%|██████████| 1399/1399 [00:13<00:00, 103.65it/s]\n",
            "100%|██████████| 8960/8960 [01:41<00:00, 88.14it/s]\n",
            "100%|██████████| 1399/1399 [00:12<00:00, 114.69it/s]\n",
            "100%|██████████| 8960/8960 [01:40<00:00, 88.98it/s] \n",
            "100%|██████████| 1399/1399 [00:14<00:00, 99.29it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term_idfs = compute_idf(corpus, vocab)"
      ],
      "metadata": {
        "id": "b1_W6mEFGAm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b9d1a2-16a4-4a20-dacc-b5be7b671d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8960/8960 [01:45<00:00, 85.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_query_vector(query_toks, scheme, term_idfs, vocab):\n",
        "    vocab_len = len(vocab)\n",
        "    num_query_toks = len(query_toks) \n",
        "    query_vector = [0] * vocab_len \n",
        "    query_tfs = {} \n",
        "    for i in range(num_query_toks):\n",
        "        query_tfs[query_toks[i]] = 0\n",
        "    for i in range(num_query_toks):\n",
        "        query_tfs[query_toks[i]] += 1\n",
        "    \n",
        "    x = 0\n",
        "    for i in query_toks:\n",
        "        term_tf_weight = tf_weight(i, query_tfs, scheme)\n",
        "        query_vector[x] = term_tf_weight * term_idfs.get(i, 0)\n",
        "        x+=1   \n",
        "    return query_vector"
      ],
      "metadata": {
        "id": "21a6M4IzCAKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_top_5(arr):\n",
        "    arr = np.array(arr)\n",
        "    return (-arr).argsort()[:10]"
      ],
      "metadata": {
        "id": "8HTIfcdENmLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'experimental slipstream sHear in simple'\n",
        "query = preprocess(query).split()\n",
        "q_vec_binary = gen_query_vector(query, 'binary', term_idfs, vocab)\n",
        "q_vec_raw = gen_query_vector(query, 'raw', term_idfs, vocab)\n",
        "q_vec_tf = gen_query_vector(query, 'tf', term_idfs, vocab)\n",
        "q_vec_log_norm = gen_query_vector(query, 'log_norm', term_idfs, vocab)\n",
        "q_vec_double_norm = gen_query_vector(query, 'double_norm', term_idfs, vocab)"
      ],
      "metadata": {
        "id": "eideAE1KC0y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_scores = np.dot(np.array(binary_tf_idf), np.array(q_vec_binary)) \n",
        "raw_scores = np.dot(np.array(raw_tf_idf), np.array(q_vec_raw)) \n",
        "tf_scores = np.dot(np.array(tf_tf_idf), np.array(q_vec_tf)) \n",
        "log_norm_scores = np.dot(np.array(log_norm_tf_idf), np.array(q_vec_log_norm)) \n",
        "double_norm_scores = np.dot(np.array(double_norm_tf_idf), np.array(q_vec_double_norm)) "
      ],
      "metadata": {
        "id": "qHCXEJhFGVt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Binary top 5\")\n",
        "top_5 = calc_top_5(binary_scores)\n",
        "for i in top_5:\n",
        "    print(i, binary_scores[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXeK1OD5N01M",
        "outputId": "51cb5ef9-8c57-4223-9a97-72ccf57cee33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary top 5\n",
            "492 5.515225426973093\n",
            "1163 5.515225426973093\n",
            "429 5.515225426973093\n",
            "481 3.748478388203395\n",
            "82 2.7863964844710054\n",
            "398 1.4490950836895478\n",
            "639 1.4490950836895478\n",
            "892 1.4490950836895478\n",
            "1059 1.4490950836895478\n",
            "11 1.4490950836895478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Raw top 5\")\n",
        "top_5 = calc_top_5(raw_scores)\n",
        "for i in top_5:\n",
        "    print(i, raw_scores[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IedQNFDGOHT4",
        "outputId": "9d3786da-52eb-419a-8d2e-484151064b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw top 5\n",
            "429 27.576127134865466\n",
            "492 11.030450853946187\n",
            "1163 11.030450853946187\n",
            "481 3.748478388203395\n",
            "82 2.7863964844710054\n",
            "398 1.4490950836895478\n",
            "639 1.4490950836895478\n",
            "892 1.4490950836895478\n",
            "1059 1.4490950836895478\n",
            "11 1.4490950836895478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Term Frequency top 5\")\n",
        "top_5 = calc_top_5(tf_scores)\n",
        "for i in top_5:\n",
        "    print(i, tf_scores[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va2tdWUdOK3d",
        "outputId": "f18fefda-7d4d-4bd0-9372-b730714da33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency top 5\n",
            "1163 0.056277810479317285\n",
            "429 0.053442106850514474\n",
            "492 0.05106690210160272\n",
            "82 0.00849511123314331\n",
            "1331 0.004364744227980565\n",
            "1167 0.004164066332441229\n",
            "481 0.0040567947924279165\n",
            "11 0.003937758379591162\n",
            "639 0.002058373698422653\n",
            "892 0.0020238758152088658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Log Normalized top 5\")\n",
        "top_5 = calc_top_5(log_norm_scores)\n",
        "for i in top_5:\n",
        "    print(i, log_norm_scores[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNQuoWgdONPT",
        "outputId": "319d789d-3e34-4e11-af8f-9e62fc072364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Normalized top 5\n",
            "429 1.2919242799842412\n",
            "492 0.7921397455378736\n",
            "1163 0.7921397455378736\n",
            "481 0.33968358155737155\n",
            "82 0.2525006254438148\n",
            "398 0.13131563185582804\n",
            "639 0.13131563185582804\n",
            "892 0.13131563185582804\n",
            "1059 0.13131563185582804\n",
            "11 0.13131563185582804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Double Normalized top 5\")\n",
        "top_5 = calc_top_5(double_norm_scores)\n",
        "for i in top_5:\n",
        "    print(i, double_norm_scores[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOS00xFnORoY",
        "outputId": "a007490b-11aa-4930-a8a3-3c084c974d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Double Normalized top 5\n",
            "492 9.507210405155067\n",
            "429 9.507210405155067\n",
            "1163 8.128404048411795\n",
            "481 7.06197089068547\n",
            "82 7.028237340115621\n",
            "11 6.991113538950112\n",
            "1167 6.930734577129714\n",
            "1331 6.894507200037475\n",
            "639 6.870355615309316\n",
            "892 6.870355615309316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_coefficient(doc, query):    \n",
        "    doc_set = set(doc)\n",
        "    query_set = set(query)\n",
        "        \n",
        "    intersection = len(doc_set.intersection(query_set))\n",
        "    union = len(doc_set.union(query_set))    \n",
        "    jaccard_coeff = intersection / union\n",
        "    \n",
        "    return jaccard_coeff"
      ],
      "metadata": {
        "id": "LeN9BD2MMHYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jcs = []\n",
        "query = 'experimental slipstream shear simple'\n",
        "query = query.lower().split()\n",
        "for i in corpus:    \n",
        "    jcs.append(jaccard_coefficient(i.split(), query))"
      ],
      "metadata": {
        "id": "opr5fVJQMhSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Jaccard coefficient\")\n",
        "top_5 = calc_top_5(jcs)\n",
        "for i in top_5:\n",
        "    print(i, jcs[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3xaJ_GlOWGc",
        "outputId": "9deb87ad-569c-4366-b420-ae07f65f32ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard coefficient\n",
            "838 0.11764705882352941\n",
            "959 0.08\n",
            "532 0.0625\n",
            "611 0.061224489795918366\n",
            "501 0.061224489795918366\n",
            "705 0.06060606060606061\n",
            "986 0.05405405405405406\n",
            "657 0.05405405405405406\n",
            "781 0.05263157894736842\n",
            "878 0.05263157894736842\n"
          ]
        }
      ]
    }
  ]
}